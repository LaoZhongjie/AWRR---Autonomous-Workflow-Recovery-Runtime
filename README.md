# AWRR — Autonomous Workflow Recovery Runtime

Production-style reliability primitives for **multi-step agent/tool workflows**: fault injection, recovery policies, checkpoint/rollback, Saga compensation, learning-from-traces, and a repeatable evaluation harness.

This repository is a **research-grade, runnable prototype**: deterministic mock tools + controlled failures + metrics/leaderboard, so you can measure reliability tradeoffs (success rate, recovery cost, MTTR, human escalation, rollback safety) instead of debating them.

---

## Why this matters (in practice)

Most agent systems fail the same way in production: **one tool call fails → the whole workflow collapses**, often with unbounded retries, inconsistent side-effects, and expensive human intervention.

AWRR focuses on the “runtime” layer:

- Make failures **observable** (trace events, budgets, state hashing)
- Make recovery **policy-driven** (retry / rollback / escalate)
- Make side-effects **recoverable** (Saga compensation)
- Make progress **measurable and comparable** (benchmarks + baselines + metrics)
- Make recovery **improve over time** (lightweight memory that learns from trajectories)

---

## What’s in the repo (highlights)

- **Fault taxonomy + controlled injection** (transient/persistent/semantic/cascade) with multiple modes (`once`, `per_attempt`, `persistent`, `stateful_conflict`) in `mock_api.py`.
- **Baselines B0–B4** in `baselines.py`:
  - `B0` no recovery
  - `B1` naive retry
  - `B2` rule-based recovery
  - `B3` diagnosis-driven recovery (mock LLM interface included)
  - `B4` diagnosis + **memory-based policy** (learns from prior failures)
- **Saga compensation** for non-idempotent side-effects (`lock_inventory`, `process_payment`) with rollback stack in `saga.py` + `runner.py`.
- **Evaluation harness**:
  - Metrics: WCR/RR/MTTR/RCO/HIR/UAR/SRR in `metrics.py`
  - Leaderboard + comparisons in `leaderboard.py`
  - RCA accuracy (layer/action) in `rca_eval.py`
  - Learning curve for B4 in `learning_eval.py`

> Note: LLM calls are **simulated** (no external API required). `DiagnosisAgent(mode="llm")` is a placeholder that falls back to mock diagnosis.

---

## Quickstart

Requirements: Python 3 (repo has been run on Python 3.13).

### 1) Run the full baseline suite (B1–B4) + leaderboard

```bash
bash run_baselines.sh
```

Override defaults:

```bash
SEED=42 N_TASKS=200 bash run_baselines.sh
```

Outputs land in `results/<timestamp>/`:

- `tasks.jsonl` generated benchmark tasks
- `traces_B*.jsonl` per-baseline traces
- `leaderboard.txt`, `leaderboard.csv` summary
- `metrics_rates.png`, `metrics_costs.png` (if `matplotlib` is installed)

### 2) Generate tasks (with a chosen fault profile)

```bash
python task_generator.py --n 100 --seed 42 --out tasks.jsonl --fault-profile balanced
```

Two presets:

- `balanced`: mixed faults across layers
- `separation`: tuned to amplify differences between recovery strategies

### 3) Run a single baseline + compute metrics

```bash
python baselines.py --tasks tasks.jsonl --mode B3 --seed 42 --out traces_B3.jsonl
python metrics.py --traces traces_B3.jsonl --baseline B3 --details
```

### 4) Evaluate diagnosis quality (RCA)

```bash
python rca_eval.py --traces traces_B3.jsonl --level task
```

### 5) Learning curve (B4)

```bash
python learning_eval.py --tasks tasks.jsonl --batch-size 10 --memory memory_bank.json \
  --out-history learning_curve.json --out-traces traces_B4_learning.jsonl
```

Optional plot:

```bash
python plot_learning.py --history learning_curve.json --out learning_curve.png
```

### 6) Saga compensation evaluation (SRR)

Baselines don’t run Saga; Saga is evaluated via `runner.py`:

```bash
python runner.py --tasks tasks.jsonl --out traces_no_saga.jsonl --no-saga
python runner.py --tasks tasks.jsonl --out traces_saga.jsonl --saga
python phase3_eval.py --no-saga traces_no_saga.jsonl --saga traces_saga.jsonl
```

---

## A concrete result snapshot (example run)

The repo includes an example run under `results/` and a `leaderboard.csv` at the root (generated by `leaderboard.py`).

On one sample run (seeded, 100 tasks, balanced faults), the baselines show:

- Rule-based recovery (`B2`) improves completion vs naive retry (`B1`) while reducing cost overhead (RCO).
- Diagnosis-driven (`B3`) adds incremental completion/recovery improvements over `B2`, at the cost of extra MTTR (diagnosis latency in this prototype).
- Memory+diagnosis (`B4`) can reduce diagnosis calls vs `B3` while keeping similar completion (depends on task mix + memory threshold).

The point isn’t a single number — it’s that the runtime makes these tradeoffs **repeatable and measurable**.

---

## Design notes (for reviewers)

- **WorldState hashing**: state is hashable to detect “stuck” loops and to build similarity signatures for memory-based recovery (`state.py`, `learning.py`).
- **Budget-aware execution**: each task has a token/time/tool-call budget; recovery is guarded to avoid runaway retries (`baselines.py`, `runner.py`).
- **Stateful conflict fault**: `stateful_conflict` persists until a rollback is observed in the audit log — a simple but effective way to test whether rollback logic is actually executed (`mock_api.py`).
- **Rollback safety**: Saga compensation is evaluated via **SRR (Safe Rollback Rate)** using a consistency oracle (`oracle_checker.py`, `metrics.py`).

---

## Repository map

Core runtime & recovery:

- `runner.py` — workflow runner with Saga compensation support (SRR evaluation)
- `saga.py` — compensation transaction stack + rollback execution
- `state.py` — `WorldState`, budgets, `TraceEvent`
- `mock_api.py` — deterministic mock tools + fault injection

Baselines & learning:

- `baselines.py` — B0–B4 recovery strategies
- `diagnosis.py` — diagnosis agent (mock + LLM placeholder)
- `learning.py` — fault signature + memory bank

Benchmark & evaluation:

- `task_generator.py` — task and fault generation
- `metrics.py` — metrics computation
- `leaderboard.py` — leaderboard + comparisons + CSV export
- `rca_eval.py` — RCA / action accuracy evaluation
- `learning_eval.py` — learning efficiency evaluation
- `visualize_metrics.py` — plots (optional `matplotlib`)

---

## How to talk about this in interviews (one-liners)

- Built a **reliability runtime** for agent workflows: checkpoint/rollback, recovery policies, and Saga compensation for side-effects.
- Designed a **controlled fault injection benchmark** and a reproducible evaluation harness with reliability + cost metrics.
- Implemented **diagnosis-driven** and **learning-based** recovery baselines; measured improvements and tradeoffs via leaderboard metrics.

---

## More detail

See `AWRR.md` for the full project write-up (architecture, taxonomy, metrics definitions, experiment methodology, and roadmap).

